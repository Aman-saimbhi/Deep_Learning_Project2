{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Additional Dependencies\n!pip install barbar torchsummary","metadata":{"execution":{"iopub.status.busy":"2022-05-18T00:53:04.7989Z","iopub.execute_input":"2022-05-18T00:53:04.799646Z","iopub.status.idle":"2022-05-18T00:53:12.03109Z","shell.execute_reply.started":"2022-05-18T00:53:04.799349Z","shell.execute_reply":"2022-05-18T00:53:12.030003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport copy\nimport pickle\nfrom barbar import Bar\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport scipy\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom PIL import Image\nimport cv2\n%matplotlib inline\n\nimport torch\nimport torchvision\nimport pickle\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.dataset import Dataset\nfrom torchvision import transforms\nfrom torchsummary import summary\n\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport gc\nRANDOMSTATE = 0\n\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-18T00:53:12.034015Z","iopub.execute_input":"2022-05-18T00:53:12.034448Z","iopub.status.idle":"2022-05-18T00:53:12.052991Z","shell.execute_reply.started":"2022-05-18T00:53:12.034387Z","shell.execute_reply":"2022-05-18T00:53:12.05201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2022-05-18T00:53:12.054989Z","iopub.execute_input":"2022-05-18T00:53:12.055816Z","iopub.status.idle":"2022-05-18T00:53:12.06294Z","shell.execute_reply.started":"2022-05-18T00:53:12.055773Z","shell.execute_reply":"2022-05-18T00:53:12.062029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datasetPath = Path('/kaggle/input/cbir-dataset/dataset/')\ndf = pd.DataFrame()\n\ndf['image'] = [f for f in os.listdir(datasetPath) if os.path.isfile(os.path.join(datasetPath, f))]\ndf['image'] = '/kaggle/input/cbir-dataset/dataset/' + df['image'].astype(str)\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T00:53:12.064896Z","iopub.execute_input":"2022-05-18T00:53:12.065775Z","iopub.status.idle":"2022-05-18T00:53:14.049868Z","shell.execute_reply.started":"2022-05-18T00:53:12.065733Z","shell.execute_reply":"2022-05-18T00:53:14.049019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CBIRDataset(Dataset):\n    def __init__(self, dataFrame):\n        self.dataFrame = dataFrame\n        \n        self.transformations = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n            transforms.RandomErasing(p=0.7, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)\n        ])\n    \n    def __getitem__(self, key):\n        if isinstance(key, slice):\n            raise NotImplementedError('slicing is not supported')\n        \n        row = self.dataFrame.iloc[key]\n        image = self.transformations(Image.open(row['image']))\n        return image\n    \n    def __len__(self):\n        return len(self.dataFrame.index)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T00:53:14.053624Z","iopub.execute_input":"2022-05-18T00:53:14.054311Z","iopub.status.idle":"2022-05-18T00:53:14.066651Z","shell.execute_reply.started":"2022-05-18T00:53:14.054271Z","shell.execute_reply":"2022-05-18T00:53:14.065601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_data(DF):\n    trainDF, validateDF = train_test_split(DF, test_size=0.15, random_state=RANDOMSTATE)\n    train_set = CBIRDataset(trainDF)\n    validate_set = CBIRDataset(validateDF)\n    \n    return train_set, validate_set","metadata":{"execution":{"iopub.status.busy":"2022-05-18T00:53:14.070829Z","iopub.execute_input":"2022-05-18T00:53:14.071222Z","iopub.status.idle":"2022-05-18T00:53:14.081089Z","shell.execute_reply.started":"2022-05-18T00:53:14.071181Z","shell.execute_reply":"2022-05-18T00:53:14.079136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ConvAutoencoder(nn.Module):\n    def __init__(self):\n        super(ConvAutoencoder, self).__init__()\n        self.encoder = nn.Sequential(# in- (N,3,512,512)\n            \n            nn.Conv2d(in_channels=3, \n                      out_channels=16, \n                      kernel_size=(3,3), \n                      stride=3, \n                      padding=1),  # (32,16,171,171)\n            nn.ReLU(True),\n            nn.MaxPool2d(2, stride=2),  # (N,16,85,85)\n            \n            nn.Conv2d(in_channels=16, \n                      out_channels=8, \n                      kernel_size=(3,3), \n                      stride=2, \n                      padding=1),  # (N,8,43,43)\n            nn.ReLU(True),\n            nn.MaxPool2d(2, stride=1)  # (N,8,42,42)\n        )\n        self.decoder = nn.Sequential(\n            \n            nn.ConvTranspose2d(in_channels = 8, \n                               out_channels=16, \n                               kernel_size=(3,3), \n                               stride=2),  # (N,16,85,85)\n            nn.ReLU(True),\n \n            nn.ConvTranspose2d(in_channels=16, \n                               out_channels=8, \n                               kernel_size=(5,5), \n                               stride=3, \n                               padding=1),  # (N,8,255,255)\n            nn.ReLU(True),\n\n            nn.ConvTranspose2d(in_channels=8, \n                               out_channels=3, \n                               kernel_size=(6,6), \n                               stride=2, \n                               padding=1),  # (N,3,512,512)\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-05-18T00:53:14.082937Z","iopub.execute_input":"2022-05-18T00:53:14.08387Z","iopub.status.idle":"2022-05-18T00:53:14.103599Z","shell.execute_reply.started":"2022-05-18T00:53:14.083772Z","shell.execute_reply":"2022-05-18T00:53:14.102534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ConvAutoencoder_v2(nn.Module):\n    def __init__(self):\n        super(ConvAutoencoder_v2, self).__init__()\n        self.encoder = nn.Sequential(# in- (N,3,512,512)\n            \n            nn.Conv2d(in_channels=3, \n                      out_channels=64, \n                      kernel_size=(3,3), \n                      stride=1, \n                      padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=64, \n                      out_channels=64, \n                      kernel_size=(3,3), \n                      stride=1, \n                      padding=1),\n            nn.ReLU(True),\n            nn.MaxPool2d(2, stride=2), \n            \n            nn.Conv2d(in_channels=64, \n                      out_channels=128, \n                      kernel_size=(3,3), \n                      stride=2, \n                      padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=128, \n                      out_channels=128, \n                      kernel_size=(3,3), \n                      stride=1, \n                      padding=0), \n            nn.ReLU(True),\n            nn.MaxPool2d(2, stride=2), \n            \n            nn.Conv2d(in_channels=128, \n                      out_channels=256, \n                      kernel_size=(3,3), \n                      stride=2, \n                      padding=1), \n            nn.ReLU(True),\n            nn.Conv2d(in_channels=256, \n                      out_channels=256, \n                      kernel_size=(3,3), \n                      stride=1, \n                      padding=1), \n            nn.ReLU(True),\n            nn.Conv2d(in_channels=256, \n                      out_channels=256, \n                      kernel_size=(3,3), \n                      stride=1, \n                      padding=1), \n            nn.ReLU(True),\n            nn.MaxPool2d(2, stride=2) \n        )\n        self.decoder = nn.Sequential(\n            \n            nn.ConvTranspose2d(in_channels = 256, \n                               out_channels=256, \n                               kernel_size=(3,3), \n                               stride=1,\n                              padding=1), \n \n            nn.ConvTranspose2d(in_channels=256, \n                               out_channels=256, \n                               kernel_size=(3,3), \n                               stride=1, \n                               padding=1),  \n            nn.ReLU(True),\n\n            nn.ConvTranspose2d(in_channels=256, \n                               out_channels=128, \n                               kernel_size=(3,3), \n                               stride=2, \n                               padding=0),  \n            \n            nn.ConvTranspose2d(in_channels=128, \n                               out_channels=64, \n                               kernel_size=(3,3), \n                               stride=2, \n                               padding=1),  \n            nn.ReLU(True),\n            nn.ConvTranspose2d(in_channels=64, \n                               out_channels=32, \n                               kernel_size=(3,3), \n                               stride=2, \n                               padding=1), \n            \n            nn.ConvTranspose2d(in_channels=32, \n                               out_channels=32, \n                               kernel_size=(3,3), \n                               stride=2, \n                               padding=1),  \n            nn.ReLU(True),\n            \n            nn.ConvTranspose2d(in_channels=32, \n                               out_channels=3, \n                               kernel_size=(4,4), \n                               stride=2, \n                               padding=2),  \n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-05-18T00:53:14.105647Z","iopub.execute_input":"2022-05-18T00:53:14.106719Z","iopub.status.idle":"2022-05-18T00:53:14.138232Z","shell.execute_reply.started":"2022-05-18T00:53:14.106599Z","shell.execute_reply":"2022-05-18T00:53:14.137335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(ConvAutoencoder_v2().to(device),(3,512,512))","metadata":{"execution":{"iopub.status.busy":"2022-05-18T00:53:14.140182Z","iopub.execute_input":"2022-05-18T00:53:14.140746Z","iopub.status.idle":"2022-05-18T00:53:14.216767Z","shell.execute_reply.started":"2022-05-18T00:53:14.140706Z","shell.execute_reply":"2022-05-18T00:53:14.210624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_ckpt(checkpoint_fpath, model, optimizer):\n    \n    # load check point\n    checkpoint = torch.load(checkpoint_fpath)\n\n    # initialize state_dict from checkpoint to model\n    model.load_state_dict(checkpoint['model_state_dict'])\n\n    # initialize optimizer from checkpoint to optimizer\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\n    # initialize valid_loss_min from checkpoint to valid_loss_min\n    #valid_loss_min = checkpoint['valid_loss_min']\n\n    # return model, optimizer, epoch value, min validation loss \n    return model, optimizer, checkpoint['epoch']\n\ndef save_checkpoint(state, filename):\n    \"\"\"Save checkpoint if a new best is achieved\"\"\"\n    print (\"=> Saving a new best\")\n    torch.save(state, filename)  # save checkpoint\n    \ndef train_model(model,  \n                criterion, \n                optimizer, \n                #scheduler, \n                num_epochs):\n    since = time.time()\n    \n    train_loss_list = []\n    test_loss_list = []\n    \n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_loss = np.inf\n\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n\n            # Iterate over data.\n            for idx,inputs in enumerate(Bar(dataloaders[phase])):\n                inputs = inputs.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    loss = criterion(outputs, inputs)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n            #if phase == 'train':\n            #    scheduler.step()\n\n            epoch_loss = running_loss / dataset_sizes[phase]\n\n            print('{} Loss: {:.4f}'.format(\n                phase, epoch_loss))\n            \n            if phase=='train':\n                train_loss_list.append(epoch_loss)\n            else:\n                test_loss_list.append(epoch_loss)\n\n            # deep copy the model\n            if phase == 'val' and epoch_loss < best_loss:\n                best_loss = epoch_loss\n                best_model_wts = copy.deepcopy(model.state_dict())\n                save_checkpoint(state={   \n                                    'epoch': epoch,\n                                    'state_dict': model.state_dict(),\n                                    'best_loss': best_loss,\n                                    'optimizer_state_dict':optimizer.state_dict()\n                                },filename='ckpt_epoch_{}.pt'.format(epoch))\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed // 60, time_elapsed % 60))\n    print('Best val Loss: {:4f}'.format(best_loss))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    with open(\"train_loss\", \"wb\") as fp:   #Pickling\n        pickle.dump(train_loss_list, fp)\n    with open(\"test_loss\", \"wb\") as fp:   #Pickling\n        pickle.dump(test_loss_list, fp)\n#     with open(\"test\", \"rb\") as fp:   # Unpickling\n#         b = pickle.load(fp)\n    \n    return model, optimizer, epoch_loss","metadata":{"execution":{"iopub.status.busy":"2022-05-18T00:53:14.220665Z","iopub.execute_input":"2022-05-18T00:53:14.221119Z","iopub.status.idle":"2022-05-18T00:53:14.25216Z","shell.execute_reply.started":"2022-05-18T00:53:14.221073Z","shell.execute_reply":"2022-05-18T00:53:14.251123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 200\nNUM_BATCHES = 32\nRETRAIN = False\n\ntrain_set, validate_set = prepare_data(DF=df)\n\ndataloaders = {'train': DataLoader(train_set, batch_size=NUM_BATCHES, shuffle=True, num_workers=1) ,\n                'val':DataLoader(validate_set, batch_size=NUM_BATCHES, num_workers=1)\n                }\n\ndataset_sizes = {'train': len(train_set),'val':len(validate_set)}\n\nmodel = ConvAutoencoder_v2().to(device)\n\ncriterion = nn.MSELoss()\n# Observe that all parameters are being optimized\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n# Decay LR by a factor of 0.1 every 7 epochs\n#exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T00:53:14.255899Z","iopub.execute_input":"2022-05-18T00:53:14.25621Z","iopub.status.idle":"2022-05-18T00:53:14.302938Z","shell.execute_reply.started":"2022-05-18T00:53:14.256173Z","shell.execute_reply":"2022-05-18T00:53:14.301964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# If re-training is required:\n# Load the old model\nif RETRAIN == True:\n    # load the saved checkpoint\n    model, optimizer, start_epoch = load_ckpt('../input/cbirpretrained/conv_autoencoder.pt', model, optimizer)\n    print('Checkpoint Loaded')","metadata":{"execution":{"iopub.status.busy":"2022-05-18T00:53:14.3041Z","iopub.execute_input":"2022-05-18T00:53:14.304387Z","iopub.status.idle":"2022-05-18T00:53:14.310071Z","shell.execute_reply.started":"2022-05-18T00:53:14.304361Z","shell.execute_reply":"2022-05-18T00:53:14.308721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model, optimizer, loss = train_model(model=model, \n                    criterion=criterion, \n                    optimizer=optimizer, \n                    #scheduler=exp_lr_scheduler,\n                    num_epochs=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T00:53:14.311714Z","iopub.execute_input":"2022-05-18T00:53:14.312254Z","iopub.status.idle":"2022-05-18T00:54:43.399704Z","shell.execute_reply.started":"2022-05-18T00:53:14.312203Z","shell.execute_reply":"2022-05-18T00:54:43.39737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the Trained Model\ntorch.save({\n            'epoch': EPOCHS,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': loss,\n            }, 'conv_autoencoderv2_200ep.pt')","metadata":{"execution":{"iopub.status.busy":"2022-05-18T00:54:43.404322Z","iopub.execute_input":"2022-05-18T00:54:43.404816Z","iopub.status.idle":"2022-05-18T00:54:43.52723Z","shell.execute_reply.started":"2022-05-18T00:54:43.404768Z","shell.execute_reply":"2022-05-18T00:54:43.526055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformations = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ])","metadata":{"execution":{"iopub.status.busy":"2022-05-18T00:54:43.528573Z","iopub.execute_input":"2022-05-18T00:54:43.528949Z","iopub.status.idle":"2022-05-18T00:54:43.540812Z","shell.execute_reply.started":"2022-05-18T00:54:43.528912Z","shell.execute_reply":"2022-05-18T00:54:43.539641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T00:54:43.542887Z","iopub.execute_input":"2022-05-18T00:54:43.543424Z","iopub.status.idle":"2022-05-18T00:54:43.553235Z","shell.execute_reply.started":"2022-05-18T00:54:43.543366Z","shell.execute_reply":"2022-05-18T00:54:43.552167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_latent_features(images, transformations):\n    \n    latent_features = np.zeros((4738,256,16,16))\n    \n    for i,image in enumerate(tqdm(images)):\n        tensor = transformations(Image.open(image)).to(device)\n        latent_features[i] = model.encoder(tensor.unsqueeze(0)).cpu().detach().numpy()\n        \n    del tensor\n    gc.collect()\n    return latent_features","metadata":{"execution":{"iopub.status.busy":"2022-05-18T00:54:43.554839Z","iopub.execute_input":"2022-05-18T00:54:43.555269Z","iopub.status.idle":"2022-05-18T00:54:43.571827Z","shell.execute_reply.started":"2022-05-18T00:54:43.555215Z","shell.execute_reply":"2022-05-18T00:54:43.570458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images = df.image.values\nlatent_features = get_latent_features(images, transformations)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T00:54:43.573449Z","iopub.execute_input":"2022-05-18T00:54:43.574715Z","iopub.status.idle":"2022-05-18T00:55:49.844591Z","shell.execute_reply.started":"2022-05-18T00:54:43.574671Z","shell.execute_reply":"2022-05-18T00:55:49.843483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indexes = list(range(0, 4738))\nfeature_dict = dict(zip(indexes,latent_features))\nindex_dict = {'indexes':indexes,'features':latent_features}","metadata":{"execution":{"iopub.status.busy":"2022-05-18T00:55:49.846673Z","iopub.execute_input":"2022-05-18T00:55:49.847111Z","iopub.status.idle":"2022-05-18T00:55:49.866284Z","shell.execute_reply.started":"2022-05-18T00:55:49.847057Z","shell.execute_reply":"2022-05-18T00:55:49.865081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# write the data dictionary to disk\nwith open('features.pkl', \"wb\") as f:\n   f.write(pickle.dumps(index_dict))","metadata":{"execution":{"iopub.status.busy":"2022-05-18T00:55:49.872209Z","iopub.execute_input":"2022-05-18T00:55:49.873968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def euclidean(a, b):\n    # compute and return the euclidean distance between two vectors\n    return np.linalg.norm(a - b)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cosine_distance(a,b):\n    return scipy.spatial.distance.cosine(a, b)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def perform_search(queryFeatures, index, maxResults=64):\n\n    results = []\n\n    for i in range(0, len(index[\"features\"])):\n        d = euclidean(queryFeatures, index[\"features\"][i])\n        results.append((d, i))\n    \n    # sort the results and grab the top ones\n    results = sorted(results)[:maxResults]\n    # return the list of results\n    return results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_montages(image_list, image_shape, montage_shape):\n\n    if len(image_shape) != 2:\n        raise Exception('image shape must be list or tuple of length 2 (rows, cols)')\n    if len(montage_shape) != 2:\n        raise Exception('montage shape must be list or tuple of length 2 (rows, cols)')\n    image_montages = []\n    # start with black canvas to draw images onto\n    montage_image = np.zeros(shape=(image_shape[1] * (montage_shape[1]), image_shape[0] * montage_shape[0], 3),\n                          dtype=np.uint8)\n    cursor_pos = [0, 0]\n    start_new_img = False\n    for img in image_list:\n        if type(img).__module__ != np.__name__:\n            raise Exception('input of type {} is not a valid numpy array'.format(type(img)))\n        start_new_img = False\n        img = cv2.resize(img, image_shape)\n        # draw image to black canvas\n        montage_image[cursor_pos[1]:cursor_pos[1] + image_shape[1], cursor_pos[0]:cursor_pos[0] + image_shape[0]] = img\n        cursor_pos[0] += image_shape[0]  # increment cursor x position\n        if cursor_pos[0] >= montage_shape[0] * image_shape[0]:\n            cursor_pos[1] += image_shape[1]  # increment cursor y position\n            cursor_pos[0] = 0\n            if cursor_pos[1] >= montage_shape[1] * image_shape[1]:\n                cursor_pos = [0, 0]\n                image_montages.append(montage_image)\n                # reset black canvas\n                montage_image = np.zeros(shape=(image_shape[1] * (montage_shape[1]), image_shape[0] * montage_shape[0], 3),\n                                      dtype=np.uint8)\n                start_new_img = True\n    if start_new_img is False:\n        image_montages.append(montage_image)  # add unfinished montage\n    return image_montages","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=2,figsize=(15,15))\nqueryIdx = 3166# Input Index for which images \nMAX_RESULTS = 10\n\n\nqueryFeatures = latent_features[queryIdx]\nresults = perform_search(queryFeatures, index_dict, maxResults=MAX_RESULTS)\nimgs = []\n\n# loop over the results\nfor (d, j) in results:\n    img = np.array(Image.open(images[j]))\n    print(j)\n    imgs.append(img)\n\n# display the query image\nax[0].imshow(np.array(Image.open(images[queryIdx])))\n\n# build a montage from the results and display it\nmontage = build_montages(imgs, (512, 512), (5, 2))[0]\nax[1].imshow(montage)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}